{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5109cc-b7cf-4804-bd2a-a885ea1abf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yash\n",
      "[nltk_data]     Raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yash\n",
      "[nltk_data]     Raj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from ./scrapedData\\authors-20240801-233645.json, containing 40959 items.\n",
      "Data loaded from ./scrapedData\\papers-20240731-170433.json, containing 12987 items.\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import webbrowser\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize Porter Stemmer and stop words\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Load data from JSON file\n",
    "def load_data_from_json(filename):\n",
    "    \"\"\"Load data from a JSON file.\"\"\"\n",
    "    filepath = os.path.join(\"./scrapedData\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Data loaded from {filepath}, containing {len(data)} items.\")\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"No file found at {filepath}\")\n",
    "        return []\n",
    "\n",
    "# Load authors and documents data\n",
    "authors = load_data_from_json(\"authors-20240801-233645.json\")\n",
    "documents = load_data_from_json(\"papers-20240731-170433.json\")\n",
    "\n",
    "# Create Inverted Index for Documents\n",
    "def create_document_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Safely convert all components to strings\n",
    "        title = doc.get('title', '') or ''\n",
    "        journal = doc.get('journal', '') or ''\n",
    "        abstract = doc.get('abstract', '') or ''\n",
    "        authors = ' '.join(doc.get('authors', [])) or ''\n",
    "        \n",
    "        # Preprocess the concatenated string\n",
    "        words = preprocess(f\"{title} {journal} {abstract} {authors}\")\n",
    "        \n",
    "        for word in words:\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word].append(i)\n",
    "    return inverted_index\n",
    "\n",
    "# Create Inverted Index for Authors\n",
    "def create_author_inverted_index(authors):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for i, author in enumerate(authors):\n",
    "        words = preprocess(author.get('name', '') + ' ' + author.get('department', ''))\n",
    "        for word in words:\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word].append(i)\n",
    "    return inverted_index\n",
    "\n",
    "# Initialize Inverted Indices\n",
    "document_inverted_index = create_document_inverted_index(documents) if documents else {}\n",
    "author_inverted_index = create_author_inverted_index(authors) if authors else {}\n",
    "\n",
    "# Define the search_and_display_results function\n",
    "def search_and_display_results():\n",
    "    global document_inverted_index, author_inverted_index\n",
    "    query = search_entry.get()\n",
    "    query_tokens = preprocess(query)\n",
    "    print(f\"Query Tokens: {query_tokens}\")\n",
    "\n",
    "    matching_docs = set()\n",
    "    matching_authors = set()\n",
    "\n",
    "    for token in query_tokens:\n",
    "        if token in document_inverted_index:\n",
    "            matching_docs.update(document_inverted_index[token])\n",
    "            print(f\"Token '{token}' found in documents: {document_inverted_index[token]}\")\n",
    "        else:\n",
    "            print(f\"Token '{token}' not found in document index.\")\n",
    "\n",
    "    for token in query_tokens:\n",
    "        if token in author_inverted_index:\n",
    "            matching_authors.update(author_inverted_index[token])\n",
    "            print(f\"Token '{token}' found in authors: {author_inverted_index[token]}\")\n",
    "        else:\n",
    "            print(f\"Token '{token}' not found in author index.\")\n",
    "\n",
    "    # Clear previous results\n",
    "    for item in result_tree.get_children():\n",
    "        result_tree.delete(item)\n",
    "\n",
    "    # Display document results in Treeview\n",
    "    if matching_docs:\n",
    "        for doc_id in matching_docs:\n",
    "            doc = documents[doc_id]\n",
    "            title = doc.get('title', 'N/A')\n",
    "            link = doc.get('link', 'N/A')\n",
    "\n",
    "            # Use `coventryAuthors` to fetch profile links if available\n",
    "            authors_data = doc.get('authors', [])\n",
    "            author_links = doc.get('coventryAuthors', [])\n",
    "            authors = ', '.join(\n",
    "                f\"{name} ({link})\" if i < len(author_links) else name\n",
    "                for i, name in enumerate(authors_data)\n",
    "            )\n",
    "\n",
    "            result_tree.insert('', tk.END, values=(title, link, authors))\n",
    "    \n",
    "    # Display author results\n",
    "    if matching_authors:\n",
    "        for author_id in matching_authors:\n",
    "            author = authors[author_id]\n",
    "            name = author.get('name', 'N/A')\n",
    "            link = author.get('profileLink', 'N/A')\n",
    "            dept = author.get('department', 'N/A')\n",
    "            result_tree.insert('', tk.END, values=(name, link, dept))\n",
    "\n",
    "    if not matching_docs and not matching_authors:\n",
    "        result_tree.insert('', tk.END, values=(\"No matches found\", \"\", \"\"))\n",
    "\n",
    "# Function to open links on double-click\n",
    "def on_treeview_click(event):\n",
    "    item = result_tree.selection()\n",
    "    if item:\n",
    "        link = result_tree.item(item, 'values')[1]\n",
    "        if link.startswith(\"http\"):\n",
    "            webbrowser.open(link)\n",
    "\n",
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Softwarica Search Engine\")\n",
    "\n",
    "search_frame = ttk.Frame(root, padding=\"10\")\n",
    "search_frame.grid(row=0, column=0, sticky=(tk.W, tk.E))\n",
    "\n",
    "search_label = ttk.Label(search_frame, text=\"Enter the query you want to search:\")\n",
    "search_label.grid(row=0, column=0, sticky=tk.W)\n",
    "\n",
    "search_entry = ttk.Entry(search_frame, width=50)\n",
    "search_entry.grid(row=0, column=1, sticky=(tk.W, tk.E))\n",
    "\n",
    "search_button = ttk.Button(search_frame, text=\"Search\", command=search_and_display_results)\n",
    "search_button.grid(row=0, column=2, sticky=tk.W)\n",
    "\n",
    "result_frame = ttk.Frame(root, padding=\"10\")\n",
    "result_frame.grid(row=1, column=0, sticky=(tk.W, tk.E))\n",
    "\n",
    "columns = (\"Title\", \"Link\", \"Authors\")\n",
    "result_tree = ttk.Treeview(result_frame, columns=columns, show='headings')\n",
    "result_tree.heading(\"Title\", text=\"Title\")\n",
    "result_tree.heading(\"Link\", text=\"Link\")\n",
    "result_tree.heading(\"Authors\", text=\"Authors\")\n",
    "\n",
    "result_tree.column(\"Title\", width=200)\n",
    "result_tree.column(\"Link\", width=200)\n",
    "result_tree.column(\"Authors\", width=150)\n",
    "\n",
    "result_tree.pack(expand=True, fill='both')\n",
    "\n",
    "result_tree.bind(\"<Double-1>\", on_treeview_click)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a954f5-e215-4257-9b1f-b3fcead05d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75f0de-ed5f-4fe0-8fee-903cca23292d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481437cc-5b4a-4073-8cba-69419bd522cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ef676-d5f8-4b4d-8bb5-98e5ba6a8ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c6c49-f663-4ecf-81be-91132d357817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232b9ce-428e-4259-b0a6-71b1311c3668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856aae9-d56f-497e-9589-7e55b2b8235a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be307c-52a9-4f32-ac68-3f6733ebed39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7280b-6fea-4307-9a09-c60093c246fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
